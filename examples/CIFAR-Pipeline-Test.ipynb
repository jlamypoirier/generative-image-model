{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 classifier from nnBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple implementation of a CIFAR-10 classifier using the nnBuilder framework. Trying to reproduce the network of https://www.tensorflow.org/tutorials/deep_cnn/ (Work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, time\n",
    "sys.path.append('../nnBuilder')\n",
    "from nnLayer import *\n",
    "from nnInput import *\n",
    "from nnTrainer import *\n",
    "from nnHandler import *\n",
    "from _nnUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=Layer(x=None,type=\"CIFAR_10\")\n",
    "preprocess_layers=[]\n",
    "preprocess_layers.append(dict(type=\"Random_Crop\",shape=[1,24,24,3]))\n",
    "preprocess_layers.append(dict(type=\"Reshape\",shape=[24,24,3]))\n",
    "preprocess_layers.append(dict(type=\"Random_Horizontal_Flip\"))\n",
    "preprocess_layers.append(dict(type=\"Random_Brightness\"))\n",
    "preprocess_layers.append(dict(type=\"Random_Contrast\"))\n",
    "preprocess_layers.append(dict(type=\"Whitening\"))\n",
    "preprocess_layers_batch=[layer.copy() for layer in preprocess_layers if layer[\"type\"]!=\"Reshape\"]\n",
    "for layer in preprocess_layers_batch:\n",
    "    layer[\"type\"]=\"Batch_\"+layer[\"type\"]\n",
    "preprocess_layers_batch[0][\"shape\"]=preprocess_layers_batch[0][\"shape\"][1:]\n",
    "#No preprocessing (40ms/batch, 95% gpu, 25% cpu)\n",
    "batch_def_0=dict(x=data,type=\"Batch_Slice\",batch=128)\n",
    "#No input pipeline (200ms/batch, 30% gpu, 35% cpu)\n",
    "batch_def_1=dict(x=data,type=\"Batch_Slice\",batch=128,out_features=preprocess_layers_batch)\n",
    "#With input pipeline and custom batch maker (memory problem if more than 1 thread, fixed?)\n",
    "#1 Thread: (200ms/batch, 30% gpu, 35% cpu)\n",
    "#2 Threads: (180ms/batch, ~40% gpu, 40% cpu)\n",
    "#3 Threads: (175ms/batch, ~45% gpu, 45% cpu)\n",
    "#Stopped Threads: (13ms/batch, ~90% gpu, 20% cpu)\n",
    "batch_def_2=dict(x=data,type=\"Batch_Slice\",batch=128,out_features=preprocess_layers_batch\n",
    "                +[dict(type=\"Pipeline\",num_threads=8)])\n",
    "#With input pipeline and custom batch maker (quick memory fix, not working)\n",
    "batch_def_3=dict(x=data,type=\"Batch_Slice\",batch=128,in_features=[dict(type=\"Pipeline\",num_threads=1)],\n",
    "                 out_features=preprocess_layers_batch+[dict(type=\"Pipeline\",num_threads=1)])\n",
    "#With input pipeline and tensorflow'batch maker (labels broken)\n",
    "batch_def_4=dict(x=data,type=\"Pipeline\",batch=128,make_batch=True,shuffle=True,in_features=preprocess_layers)\n",
    "\n",
    "batch=Layer(**batch_def_2)\n",
    "layers=[]\n",
    "layers.append(dict(type=\"Convolution\",pad=\"SAME\",window=5,stride=1,size=64,relu=True,out_features=[\n",
    "              dict(type=\"Pool\",pad=\"SAME\",window=3,stride=2,pool_type=\"max\"),\n",
    "              dict(type=\"Local_Response_Norm\")]))\n",
    "layers.append(dict(type=\"Convolution\",pad=\"SAME\",window=5,stride=1,size=64,relu=True,out_features=[\n",
    "              dict(type=\"Pool\",pad=\"SAME\",window=3,stride=2,pool_type=\"max\"),\n",
    "              dict(type=\"Local_Response_Norm\")]))\n",
    "layers.append(dict(type=\"Relu\",size=384))\n",
    "layers.append(dict(type=\"Relu\",size=192))\n",
    "layers.append(dict(type=\"Linear\",size=10,in_features=[\"Dropout\"]))\n",
    "network_def=dict(type=\"Network\",layers=layers)\n",
    "network=Layer(x=batch,**network_def)\n",
    "trainer=ClassifierTrainer(network=network,optimizer=\"adam\",finish=True)\n",
    "sess=SessManager(data,batch,network,trainer)#,input_network)\n",
    "sess.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.train(0,keep_rate=1.)\n",
    "trainer.eval_error()\n",
    "for i in range(5):\n",
    "    t0=time.time()\n",
    "    trainer.train(100,keep_rate=1.)\n",
    "    trainer.train(0,keep_rate=1.)\n",
    "    trainer.eval_error() #Training error, reaches about 10%\n",
    "    print(\"Time: %s\"%(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network.save() #The full network definition\n",
    "save=network.save() #The full network definition\n",
    "assert(Layer(x=batch,**save).save()==save) #Consistency\n",
    "print(save==network_def) #Saving adds stuff, should still be equivalent\n",
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.\n",
    "sess=tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=Layer(x=None,type=\"CIFAR_10\")\n",
    "preprocess_layers=[]\n",
    "preprocess_layers.append(dict(type=\"Random_Crop\",shape=[1,24,24,3]))\n",
    "preprocess_layers.append(dict(type=\"Reshape\",shape=[24,24,3]))\n",
    "preprocess_layers.append(dict(type=\"Random_Horizontal_Flip\"))\n",
    "preprocess_layers.append(dict(type=\"Random_Brightness\"))\n",
    "preprocess_layers.append(dict(type=\"Random_Contrast\"))\n",
    "preprocess_layers.append(dict(type=\"Whitening\"))\n",
    "preprocess_layers_batch=[layer.copy() for layer in preprocess_layers if layer[\"type\"]!=\"Reshape\"]\n",
    "for layer in preprocess_layers_batch:\n",
    "    layer[\"type\"]=\"Batch_\"+layer[\"type\"]\n",
    "preprocess_layers_batch[0][\"shape\"]=preprocess_layers_batch[0][\"shape\"][1:]\n",
    "#No preprocessing (40ms/batch, 95% gpu, 25% cpu)\n",
    "batch_def_0=dict(x=data,type=\"Batch_Slice\",batch=128)\n",
    "#No input pipeline (200ms/batch, 30% gpu, 35% cpu)\n",
    "batch_def_1=dict(x=data,type=\"Batch_Slice\",batch=128,out_features=preprocess_layers_batch)\n",
    "#With input pipeline and custom batch maker (memory problem if more than 1 thread)\n",
    "#1 Thread: (200ms/batch, 30% gpu, 35% cpu)\n",
    "#2 Threads: (180ms/batch, ~40% gpu, 40% cpu)\n",
    "#3 Threads: (175ms/batch, ~45% gpu, 45% cpu)\n",
    "#4+ Threads: OOM\n",
    "#Stopped Threads: (13ms/batch, ~90% gpu, 20% cpu)\n",
    "batch_def_2=dict(x=data,type=\"Batch_Slice\",batch=128,out_features=preprocess_layers_batch\n",
    "                +[dict(type=\"Pipeline\",num_threads=1)])\n",
    "#With input pipeline and custom batch maker (quick memory fix, not working)\n",
    "batch_def_3=dict(x=data,type=\"Batch_Slice\",batch=128,in_features=[dict(type=\"Pipeline\",num_threads=1)],\n",
    "                 out_features=preprocess_layers_batch+[dict(type=\"Pipeline\",num_threads=1)])\n",
    "#With input pipeline and tensorflow'batch maker (labels broken)\n",
    "batch_def_4=dict(x=data,type=\"Pipeline\",batch=128,make_batch=True,shuffle=True,in_features=preprocess_layers)\n",
    "\n",
    "#batch=Layer(**batch_def_1)\n",
    "layers=[]\n",
    "layers.append(dict(type=\"Convolution\",pad=\"SAME\",window=5,stride=1,size=64,relu=True,out_features=[\n",
    "              dict(type=\"Pool\",pad=\"SAME\",window=3,stride=2,pool_type=\"max\"),\n",
    "              dict(type=\"Local_Response_Norm\")]))\n",
    "layers.append(dict(type=\"Convolution\",pad=\"SAME\",window=5,stride=1,size=64,relu=True,out_features=[\n",
    "              dict(type=\"Pool\",pad=\"SAME\",window=3,stride=2,pool_type=\"max\"),\n",
    "              dict(type=\"Local_Response_Norm\")]))\n",
    "layers.append(dict(type=\"Relu\",size=384))\n",
    "layers.append(dict(type=\"Relu\",size=192))\n",
    "layers.append(dict(type=\"Linear\",size=10,in_features=[\"Dropout\"]))\n",
    "network_def=dict(type=\"Network\",layers=layers)\n",
    "#network=Layer(x=batch,**network_def)\n",
    "#trainer=ClassifierTrainer(network=network,optimizer=\"adam\",finish=True)\n",
    "sess=SessManager(data)#,batch,network,trainer)#,input_network)\n",
    "sess.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_sizes = [np.product(list(map(int, v.get_shape())))*v.dtype.size\n",
    "             for key in data.get().graph.get_all_collection_keys() for v in data.get().graph.get_collection_ref(key)]\n",
    "print(sum(var_sizes)/(1024**2), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.get().graph.get_all_collection_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.get().graph.get_collection_ref('variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph=tf.get_default_graph()\n",
    "ops=graph.get_operations()\n",
    "tensors=[t for op in ops for t in op.outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tensor in tensors:\n",
    "    print(np.product(tensor.get_shape().as_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tensor in tensors:\n",
    "    try:\n",
    "        print(sess.run(tf.shape(tensor)))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensors[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sess.run(tf.shape(tensors[30])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=tf.Variable(tf.zeros(shape=[250000000], dtype=tf.float32))\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.InteractiveSession(config=config)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "250000000*4/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(2506-350)-953*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=tf.reshape(tf.Variable(0, dtype=tf.float32,validate_shape=False),[250000000])\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.InteractiveSession(config=config)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, time\n",
    "sys.path.append('../nnBuilder')\n",
    "from nnLayer import *\n",
    "from nnInput import *\n",
    "from nnTrainer import *\n",
    "from nnHandler import *\n",
    "from _nnUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.ones([1000],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader=tf.read_file(\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.parse_tensor(reader, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sess.run(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sess.run(tf.parse_tensor(reader, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.ones(1,dtype=np.float32).tostring()\n",
    "b=tf.constant(a)\n",
    "sess.run(b)\n",
    "#sess.run(tf.parse_tensor(b, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader=tf.read_file(\"file\")\n",
    "b=sess.run(tf.decode_raw(reader, tf.float32))[3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.ones([25000000],dtype=np.float32)\n",
    "writer=tf.python_io.TFRecordWriter(\"file\")\n",
    "writer.write(a.tostring())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader=tf.read_file(\"file\")\n",
    "decode=tf.decode_raw(reader, tf.float32)[3:-1]\n",
    "var=tf.Variable(decode,validate_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.InteractiveSession(config=config)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.run().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
